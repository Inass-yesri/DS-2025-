

Introduction générale

Le dataset Heart Disease (Cleveland) est l’un des jeux de données médicaux les plus utilisés pour l’étude de la maladie coronarienne à l’aide des méthodes statistiques et de l’apprentissage automatique. Collecté à la fin des années 1970 et au début des années 1980 dans le cadre du Coronary Artery Disease Investigation, ce dataset regroupe des données cliniques standardisées provenant d’examens cardiaques réalisés sur des patients adultes. Son objectif principal est d’identifier les facteurs cliniques et biologiques permettant de prédire la présence d’une maladie coronarienne significative.
Le jeu de données Cleveland est considéré comme la référence parmi les quatre bases disponibles (Cleveland, Hungarian, Switzerland, Long Beach VA), car il s’agit de la seule base possédant des données complètes et exploitées dans la littérature scientifique. Comprendre son origine, sa méthodologie de collecte, sa population et ses limites est essentiel pour situer ces données dans leur cadre hospitalier réel.
________________________________________
Questions principales et réponses détaillées
________________________________________
1. Qui a réalisé cette étude ?
L’étude a été menée par :
•	Le Cleveland Clinic Foundation, un centre hospitalier spécialisé en cardiologie reconnu mondialement, basé à Cleveland, Ohio (USA).
•	Les chercheurs principaux incluent :
o	Dr. Robert Detrano, cardiologue spécialiste des méthodes quantitatives ;
o	Le groupe de travail Multivariate Computerized Diagnosis of Coronary Artery Disease, impliquant cardiologues, statisticiens et spécialistes en informatique médicale.
Les fichiers accompagnant le dataset font explicitement référence à “ask-detrano”, en lien direct avec le Dr Detrano.
________________________________________
2. Quand cette étude a-t-elle été menée ?
•	La collecte initiale des données a été réalisée entre 1978 et 1988 dans le cadre d’un programme de recherche sur la maladie coronarienne.
•	Le dataset a été publié au UCI Machine Learning Repository en 1988.
•	Les premières publications scientifiques associées datent de 1989–1990, notamment celles de Detrano et Janosi.
________________________________________
3. Comment les données ont-elles été recueillies ?
Les données proviennent :
1.	D’un examen clinique standardisé : âge, sexe, type de douleur thoracique (angor), tension artérielle, cholestérol, glycémie à jeun.
2.	D’examens cardiologiques :
o	électrocardiogramme (ECG) au repos,
o	épreuve d’effort,
o	dépression du segment ST,
o	pente du ST après effort.
3.	D’une angiographie coronaire, utilisée comme méthode de référence (“gold standard”) pour confirmer ou non une obstruction des artères coronaires.
Le dataset final comporte 14 variables cliniques et biologiques standardisées.
________________________________________
4. Où cette étude a-t-elle été menée ?
•	Pays : États-Unis
•	Ville : Cleveland, Ohio
•	Institution principale : Cleveland Clinic Foundation
•	Contexte : étude multicentrique internationale sur la maladie coronarienne, incluant également des données provenant de :
o	la Hongrie,
o	la Suisse,
o	l’hôpital Long Beach VA (Californie).
Seul le site de Cleveland dispose d’un dataset complet, ce qui explique son utilisation quasi universelle.
________________________________________
5. Quelle population a été étudiée ?
La population inclut :
•	303 patients dans la version originale Cleveland ;
•	Adultes, hommes et femmes, adressés pour une suspicion de maladie coronarienne ;
•	Les données finales classent les individus en :
o	0 = absence de maladie coronarienne significative,
o	1 = présence d’une maladie coronarienne (≥ 50 % de rétrécissement d’une artère coronaire).
La cohorte est considérée comme représentative des patients hospitaliers en cardiologie durant la période étudiée.
________________________________________
6. Quelles autres études ont obtenu des résultats similaires ?
De nombreuses recherches ont confirmé la pertinence prédictive de ce dataset :
•	Detrano et al. (1990) – International application of a new probability algorithm for the diagnosis of CAD
→ Précision diagnostique élevée, validée sur plusieurs pays.
•	Janosi et al. (1988) – travaux fondateurs sur l'analyse multivariée de la maladie coronarienne.
•	Chaudhary et al. (2016) – modèles SVM et réseaux neuronaux atteignant ~85–90 % de précision.
•	Fahad et al. (2020) – Random Forest et Gradient Boosting dépassant 92 % de précision.
•	Mohammed et al. (2021) – modèles modernes (XGBoost, CatBoost) avec ~94–96 % de précision.
Ces études confirment la cohérence et la robustesse clinique des 14 variables du dataset.
________________________________________
Questions complémentaires pertinentes avec réponses
________________________________________
7. Quelle est la nature des variables mesurées ?
Les caractéristiques incluent :
•	données démographiques (âge, sexe) ;
•	marqueurs cliniques (cholestérol, tension artérielle, glycémie) ;
•	caractéristiques de douleur thoracique (4 types) ;
•	ECG au repos ;
•	performances à l’épreuve d’effort ;
•	anomalies du segment ST ;
•	état post-effort ;
•	présence d’angine induite ;
•	vaisseaux colorés lors de l’angiographie.
Ces variables couvrent l’ensemble du tableau clinique de la coronaropathie.
________________________________________
8. Pourquoi ces caractéristiques sont-elles pertinentes médicalement ?
La coronaropathie se manifeste par :
•	ischémie d’effort → anomalies du ST, test d’effort positif ;
•	facteurs de risque → cholestérol élevé, âge, tabac, hypertension ;
•	symptômes cliniques → angine typique ou atypique.
Les 14 variables sont donc directement liées aux mécanismes physiopathologiques sous-jacents.
________________________________________
9. Quel est le design méthodologique de l’étude ?
Il s’agit d’une étude :
•	observ ationnelle,
•	multicentrique,
•	basée sur des données cliniques réelles,
•	utilisant l’angiographie comme référence diagnostique.
________________________________________
10. Quelles limites méthodologiques doivent être considérées ?
•	Dataset relativement ancien : pratiques cliniques des années 1980 ;
•	Pas de variables modernes (troponine, imagerie avancée) ;
•	Données manquantes dans certaines variables ;
•	Hétérogénéité partielle entre sites (Cleveland vs Hungary, etc.).
________________________________________
11. Le dataset peut-il être généralisé ?
La généralisation est raisonnable mais doit être prudente :
•	population hospitalière spécifique des années 1980,
•	forte proportion d'hommes (typique de la cardiologie à l’époque),
•	absence de données ethniques.
De nouveaux jeux de données seraient nécessaires pour valider sur une population contemporaine.
________________________________________
12. Quels biais potentiels peuvent affecter les résultats ?
•	Biais de sélection (patients hospitalisés) ;
•	Biais techniques dans la mesure du ST ou de l’ECG ;
•	Biais de centre (Cleveland = meilleur dataset, autres incomplets).
________________________________________
13. Quels modèles de machine learning sont les plus adaptés ?
Les modèles efficaces incluent :
•	Logistic Regression,
•	Random Forest,
•	Gradient Boosting,
•	SVM,
•	Neural Networks,
•	XGBoost / CatBoost.
________________________________________
14. Quelles métriques sont pertinentes en contexte cardiaque ?
•	Sensibilité (détecter les vrais malades),
•	Spécificité,
•	AUC-ROC,
•	F1-score,
•	Precision-Recall.
L’accuracy seule n’est pas suffisante en contexte médical.
________________________________________
15. Quel est l'intérêt clinique de ce dataset ?
Il permet :
•	de développer des outils d’aide au diagnostic,
•	de tester des modèles sur des données cliniques réelles,
•	d’identifier les facteurs de risque les plus liés à la coronaropathie,
•	d’améliorer la détection précoce de la maladie coronarienne.
________________________________________
Conclusion générale
Le dataset Heart Disease – Cleveland constitue une référence centrale pour l’étude du diagnostic de la maladie coronarienne. Construit à partir de données cliniques réelles, validé par angiographie, et largemen
t étudié dans la littérature médicale et informatique, il permet de tester et comparer de nombreux modèles prédictifs. Malgré ses limites liées à l'ancienneté et à un contexte hospitalier spécifique, il demeure un benchmark essentiel pour la recherche en apprentissage automatique appliquée à la cardiologie.

## MODULE 2 : STATISTIQUES POUR LA SCIENCE DES DONNÉES

### 2.1 Statistiques Descriptives

Les statistiques descriptives résument et décrivent les principales caractéristiques d'un ensemble de données.

**Mesures de tendance centrale :**

- **Moyenne** : $\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$
- **Médiane** : Valeur centrale qui divise les données en deux parties égales
- **Mode** : Valeur la plus fréquente

**Mesures de dispersion :**

- **Variance** : $s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2$
- **Écart-type** : $s = \sqrt{s^2}$
- **Étendue** : Maximum - Minimum
- **Écart interquartile (IQR)** : Q3 - Q1

**Code Python - Statistiques descriptives :**

```python  
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Charger un dataset en ligne (exemple : ventes d'une entreprise)
url = "https://raw.githubusercontent.com/datasets/gdp/master/data/gdp.csv"
df = pd.read_csv(url)

# Afficher les premières lignes
print(df.head())

# Informations sur le dataset
print(df.info())

# Statistiques descriptives
print(df.describe())

# Statistiques pour une colonne spécifique
colonne = df['Value']  # Adapter selon vos données

print(f"Moyenne: {colonne.mean():.2f}")
print(f"Médiane: {colonne.median():.2f}")
print(f"Mode: {colonne.mode()[0]:.2f}")
print(f"Écart-type: {colonne.std():.2f}")
print(f"Variance: {colonne.var():.2f}")
print(f"Min: {colonne.min():.2f}")
print(f"Max: {colonne.max():.2f}")

# Quartiles
print(f"Q1 (25%): {colonne.quantile(0.25):.2f}")
print(f"Q2 (50% - Médiane): {colonne.quantile(0.50):.2f}")
print(f"Q3 (75%): {colonne.quantile(0.75):.2f}")
print(f"IQR: {colonne.quantile(0.75) - colonne.quantile(0.25):.2f}")

```

### 2.2 Visualisation des Statistiques Descriptives
```python
# Configuration du style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

# Créer une figure avec plusieurs sous-graphiques
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Histogramme
axes[0, 0].hist(colonne, bins=30, edgecolor='black', alpha=0.7)
axes[0, 0].set_title('Distribution des valeurs', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Valeur')
axes[0, 0].set_ylabel('Fréquence')

# Ajouter la moyenne et la médiane
axes[0, 0].axvline(colonne.mean(), color='red', linestyle='--', 
                   label=f'Moyenne: {colonne.mean():.2f}')
axes[0, 0].axvline(colonne.median(), color='green', linestyle='--', 
                   label=f'Médiane: {colonne.median():.2f}')
axes[0, 0].legend()

# 2. Boxplot
axes[0, 1].boxplot(colonne.dropna(), vert=True)
axes[0, 1].set_title('Boxplot - Détection des outliers', fontsize=14, fontweight='bold')
axes[0, 1].set_ylabel('Valeur')

# 3. Densité
colonne.plot(kind='density', ax=axes[1, 0])
axes[1, 0].set_title('Courbe de densité', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Valeur')

# 4. QQ-plot (pour tester la normalité)
from scipy import stats
stats.probplot(colonne.dropna(), dist="norm", plot=axes[1, 1])
axes[1, 1].set_title('Q-Q Plot (Test de normalité)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()
```


